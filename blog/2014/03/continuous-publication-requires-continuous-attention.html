---
layout: blog
root: ../../..
author: Greg Wilson
title: "Continuous Publication Requires Continuous Attention"
date: 2014-03-29
time: "10:30:00"
category: ["Opinion"]
---
<!-- start excerpt -->
<p>
  I read
  <a href="http://blog.martinfenner.org/2014/03/10/continuous-publishing/">this post</a>
  by Martin Fenner
  a couple of weeks ago.
  His thesis is that scientific publication is still very much a manual process,
  which makes publications relatively infrequent (and fairly painful) events.
  Instead,
  we ought to strive for <em>continuous delivery</em>:
  production of the "paper" (including release of associated code and data)
  should be fully automated
  so that authors can ship whenever they want
  with relatively little effort.
</p>
<p>
  Continuous delivery is popular among software developers,
  who frequently argue it's more efficient using diagrams like this:
</p>
<!-- end excerpt -->
<p><img src="http://upload.wikimedia.org/wikipedia/commons/1/1c/Agile-vs-iterative-flow.jpg" alt="Continuous Delivery" /></p>
<p>
  What's missing here,
  though,
  is the cost to customers
  (or in the case of publishing, readers).
  Every time Mozilla releases an update to Firefox,
  millions of people have to wait thirty seconds for it to download,
  install itself,
  and restart.
  And despite the best efforts of
  <a href="http://aosabook.org/en/ffreleng.html">a world-class release engineering team</a>,
  every update will destabilize somebody, somewhere.
</p>
<p>
  Similarly,
  every time someone updates a pre-print on <a href="http://arxiv.org/">arXiv.org</a>,
  everyone who has read the original has to choose between
  ignoring the changes
  or re-reading the paper.
  In the first case they risk missing results,
  but in the second they pay an opportunity cost,
  just as users do when companies update software over and over.
</p>
<p>
  Things are worse for the readers of scientific papers.
  Release engineers can check that upgrades work for common configurations before shipping them,
  but there's no equivalent for semantic changes to papers.
  And as more scientists start communicating via blogs and twitter,
  keeping up to date with changes to things previously read will only become harder.
</p>
<p>
  I used to think that
  <a href="http://en.wikipedia.org/wiki/Automatic_summarization">automatic text summarization</a>
  would help,
  but results to date have been disappointing.
  I would welcome pointers to anyone working on new forms of scholarly publishing
  that explicitly address re-issuance.
</p>
