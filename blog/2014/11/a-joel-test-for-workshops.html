---
layout: blog
root: ../../..
author: Greg Wilson
title: "A 'Joel Test' for Grassroots Programming Groups"
date: 2014-11-03
time: "16:00:00"
category: ["Teaching"]
---
<!-- start excerpt -->
<p>
  Back during the first dot-com bubble,
  Joel Spolsky an article titled
  <a href="http://www.joelonsoftware.com/articles/fog0000000043.html">"The Joel Test: 12 Steps to Better Code"</a>
  that listed 12 questions you can ask to estimate the maturity of a software development team:
</p>
<!-- end excerpt -->
<ol>
  <li>Do you use source control?</li>
  <li>Can you make a build in one step?</li>
  <li>Do you make daily builds?</li>
  <li>Do you have a bug database?</li>
  <li>Do you fix bugs before writing new code?</li>
  <li>Do you have an up-to-date schedule?</li>
  <li>Do you have a spec?</li>
  <li>Do programmers have quiet working conditions?</li>
  <li>Do you use the best tools money can buy?</li>
  <li>Do you have testers?</li>
  <li>Do new candidates write code during their interview?</li>
  <li>Do you do hallway usability testing? </li>
</ol>
<p>
  It's completely unscientific,
  but it was also very useful and influential
  (and was in fact one of the inspirations for our
  <a href="http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001745">"Best Practices for Scientific Computing"</a>
  paper).
  In that spirit,
  I would like to present a 'Joel Test' for estimating the maturity of
  a grassroots "learn to program" project like ours.
  This list is obviously biased toward Software Carpentry,
  so I'd appreciate fixes to make it more general.
  Here are the questions:
</p>
<ol>
  <li>Are all of your lessons searchable?</li>
  <li>Does each lesson solve a problem your learners believe they have?</li>
  <li>Are you teaching principles that will still be relevant in five years?</li>
  <li>Do your instructors regularly update your lessons in a publicly-accessible version control repository?</li>
  <li>Do your instructors record and share their pedagogical content knowledge?</li>
  <li>Is there a coherent narrative running through all your lessons?</li>
  <li>Are lessons presented in short, digestible pieces with clear objectives at the start and practice exercises at the end?</li>
  <li>Do you code live when teaching?</li>
  <li>Do your setup instructions work for most learners, and do instructors know how to fix things when they don't?</li>
  <li>Do you have real-time feedback in your classroom?</li>
  <li>Do you check learners' current knowledge and skills before teaching?</li>
  <li>Do you check what learners got out of the workshop weeks or months later?</li>
  <li>Do you have a code of conduct that everyone is familiar with and that is actually enforced?</li>
  <li>Do you recruit potential new instructors from your workshops?</li>
  <li>Do you teach your instructors how to teach, and do they give each other feedback on teaching?</li>
  <li>Are new instructors mentored by more experienced peers?</li>
</ol>
<p>
  and here's how I think we're doing.
</p>
<dl>
  <dt>1. Are all of your lessons searchable?</dt>
  <dd>
    <p>
      FIXME: Can Google see the lesson content?  (Can't see into Flash, video, etc.)
    </p>
  </dd>
  <dt>2. Does each lesson solve a problem your learners believe they have?</dt>
  <dd>
    <p>
      FIXME: because if they don't, they won't stick.
    </p>
  </dd>
  <dt>3. Are you teaching principles that will still be relevant in five years?</dt>
  <dd>
    <p>
      FIXME: because specific tools come and go, but principles remain.  (Note: bait and switch is OK: offer today's specifics, smuggle in big ideas.)
    </p>
  </dd>
  <dt>4. Do your instructors regularly update your lessons in a publicly-accessible version control repository?</dt>
  <dd>
    <p>
      FIXME: open is more sustainable and yields better lessons.
    </p>
  </dd>
  <dt>5. Do your instructors record and share their pedagogical content knowledge?</dt>
  <dd>
    <p>
      FIXME: write down what examples illustrate points well, what problems learners have and how to fix them, how long lessons take, etc.
    </p>
  </dd>
  <dt>6. Is there a coherent narrative running through all your lessons?</dt>
  <dd>
    <p>
      FIXME: versus a jumble of useful but hard-to-connect facts.
    </p>
  </dd>
  <dt>7. Are lessons presented in short, digestible pieces with clear objectives at the start and practice exercises at the end?</dt>
  <dd>
    <p>
      FIXME: i.e., good lesson design.
    </p>
  </dd>
  <dt>8. Do you code live when teaching?</dt>
  <dd>
    <p>
      FIXME: which really means, do you go at your learners' pace and can *you* follow *them*?
    </p>
  </dd>
  <dt>9. Do your setup instructions work for most learners, and do instructors know how to fix things when they don't?</dt>
  <dd>
    <p>
      FIXME: i.e., have you addressed the biggest demotivator, which is "I can't even get started."
    </p>
  </dd>
  <dt>10. Do you have real-time feedback in your classroom?</dt>
  <dd>
    <p>
      FIXME: clickers, sticky notes, etherpad, whatever - are you responding to your learners?
    </p>
  </dd>
  <dt>11. Do you check learners' current knowledge and skills before teaching?</dt>
  <dd>
    <p>
      FIXME: do you know who you're actually teaching before you start?
    </p>
  </dd>
  <dt>12. Do you check what learners got out of the workshop weeks or months later?</dt>
  <dd>
    <p>
      FIXME: do you keep track of what sticks so that you can change and improve your teaching?
    </p>
  </dd>
  <dt>13. Do you have a code of conduct that everyone is familiar with and that is actually enforced?</dt>
  <dd>
    <p>
      FIXME: are your classrooms *really* open to everyone?
    </p>
  </dd>
  <dt>14. Do you recruit potential new instructors from your workshops?</dt>
  <dd>
    <p>
      FIXME: long-term sustainability.
    </p>
  </dd>
  <dt>15. Do you teach your instructors how to teach, and do they give each other feedback on teaching?</dt>
  <dd>
    <p>
      FIXME: are your instructors singing from the same songbook and helping each other improve?
    </p>
  </dd>
  <dt>16. Are new instructors mentored by more experienced peers?</dt>
  <dd>
    <p>
      FIXME: learning
    </p>
  </dd>
</dl>
<p>
  So, how is Software Carpentry doing?
</p>
<table class="table table-striped">
  <tr>
    <td align="right">1.</td>
    <td>Searchable lessons</td>
    <td align="right">1</td>
    <td></td>
  </tr>
  <tr>
    <td align="right">2.</td>
    <td>Lesson solve learners' problem</td>
    <td align="right">1</td>
    <td></td>
  </tr>
  <tr>
    <td align="right">3.</td>
    <td>Teaching principles</td>
    <td align="right">1</td>
    <td></td>
  </tr>
  <tr>
    <td align="right">4.</td>
    <td>Instructors update lessons in a public repository</td>
    <td align="right">1</td>
    <td></td>
  </tr>
  <tr>
    <td align="right">5.</td>
    <td>Instructors share PCK</td>
    <td align="right">0</td>
    <td>Our instructor's guide only captures a fraction of what we know, and isn't updated regularly.</td>
  </tr>
  <tr>
    <td align="right">6.</td>
    <td>Coherent narrative</td>
    <td align="right">0</td>
    <td>Unlike Data Carpentry, we don't use a running example throughout our lessons.</td>
  </tr>
  <tr>
    <td align="right">7.</td>
    <td>Presentation with objectives and exercises</td>
    <td align="right">0.5</td>
    <td>Ours exist, but are disjointed and incomplete.</td>
  </tr>
  <tr>
    <td align="right">8.</td>
    <td>Code live</td>
    <td align="right">1</td>
    <td></td>
  </tr>
  <tr>
    <td align="right">9.</td>
    <td>Setup instructions</td>
    <td align="right">0.5</td>
    <td>Full marks for instructions, but only part marks for instructors knowing how to debug things that go wrong.</td>
  </tr>
  <tr>
    <td align="right">10.</td>
    <td>Real-time feedback</td>
    <td align="right">1</td>
    <td></td>
  </tr>
  <tr>
    <td align="right">11.</td>
    <td>Check learners beforehand</td>
    <td align="right">1</td>
    <td></td>
  </tr>
  <tr>
    <td align="right">12.</td>
    <td>Follow-up evaluation</td>
    <td align="right">0</td>
    <td>Our lack of post-workshop assessment is our biggest weakness.</td>
  </tr>
  <tr>
    <td align="right">13.</td>
    <td>Code of conduct</td>
    <td align="right">1</td>
    <td></td>
  </tr>
  <tr>
    <td align="right">14.</td>
    <td>Recruit new instructors</td>
    <td align="right">1</td>
    <td></td>
  </tr>
  <tr>
    <td align="right">15.</td>
    <td>Mentor instructors</td>
    <td align="right">0</td>
    <td>We don't have any systematic mentoring in place.</td>
  </tr>
  <tr>
    <td align="right">16.</td>
    <td>Instructor training and feedback</td>
    <td align="right">0.5</td>
    <td>Our training doesn't cover the PCK of our lessons, and we're not systematic about peer feedback.</td>
  </tr>
  <tr>
    <td></td>
    <td><strong>Total</strong></td>
    <td align="right"><strong>10.5</strong></td>
    <td></td>
  </tr>
</table>
<p>
  10.5 out of 16 would be a 'C' at most schools,
  so we clearly have a lot of work to do.
  Once the Software Carpentry Foundation is properly launched,
  I hope we'll be able to turn our attention to the places where we fall short.
</p>
<p>
  The real aim of this rubric,
  though,
  is to help us compare what we're doing to other efforts.
  If there are things you do that you think are valuable,
  but which don't show up in this list,
  please let us know.
  Equally,
  if there are questions in this list that you think shouldn't be there
  because they're too specific to what Software Carpentry does,
  or not actually important to helping volunteers deliver high-quality training,
  please let us know that too.
</p>
