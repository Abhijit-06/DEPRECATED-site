---
layout: blog
root: ../../..
author: Greg Wilson
title: "Learning in Both Directions"
date: 2015-04-19
time: "12:00:00"
category: ["Opinion", "Assessment"]
---
<!-- start excerpt -->
<!-- end excerpt -->
I think we're talking across paradigms: what we propose is not medical trials, but culture change.  The sort of evidence that influences individuals' decisions is not primarily randomised trials; just as important are resonant experiences from trusted authorities.  (Rogers talks about this in 'Diffusion of Innovations', and we wrote about it as well in our chapter on evidence in 'Making Software'.)

We intend to evaluate two main phenomena:

- adoption of software engineering practices taught in the course in both individual practice and scientific teams (with attention both to factors affecting adoption and impact); and

- the potential of this form of presentation to build community.

We will also a look at learning outcomes, but given SC's success, this is a means to an end: what we need to demonstrate now is not "people are learning what we're teaching" but "what people are learning is changing how they do science".

Randomised trials of course participants aren't going to show this: the evaluation of the adoption of practices into scientific teams, and of community building requires quite a different type of investigation.  That's why we have proposed all the mechanisms we need (i.e., instrumentation in the course and questionnaires and interviews with participants) to gather both qualitative and quantitative evidence about individual impact (including some statistical analysis of learning outcomes).  As I mentioned before, we'll colligate different forms of data to test our observations, for example looking for correlations between certain behaviours or contextual factors and outcomes such as long-term adoption or ongoing community interaction.

We will also do longitudinal work to see what effect there might be over time.  This will include interviews and potentially questionnaires, as you suggest.  The notion of a control group is inappropriate for this: we can contrast what works with what doesn't within the population that takes the course.  However, working with two different disciplines will give us some scope for quasi-experiments.

Thus, rather than hypotheses (from a controlled experimental paradigm) we offer observations (i.e., evidence-based hypotheses, from an interpretive paradigm).  In place of randomised trials, we will connect multiple forms of data, including some statistical analysis.  Both are rigorous, but our experience, and the experience of others, is that the latter takes much fuller account of context - which is crucial in this case.  Again, the qualitative/quantitative mix is usually the most appropriate and productive approach in studies of human learning, behaviour, and culture change.

Regarding cost, the code review pilot study took at least two full months of my time (spread over more months), and as much of Greg's. That was just 12 mentors and 12 scientific teams, but it involved following their activity on the repository and mailing lists, sitting in on meetings, and interviewing them all at intervals.

If we had to cut corners, we could do a quick pre-test/post-test comparison on skills and deliver some simple numbers on learning attainment.  But as mentioned above, we already know the answers to that: most scientists learn something useful.  We could turn that into bar charts, but we want deeper insight, which means we need human scrutiny of code and communication.  Our budget for that is based on our experience with the code review study.

However, even that won't tell us about adoption into practice, drivers for and impediments to adoption, knowledge exchange in a new community, etc.  For that, we need follow-up, which needs even more time (and persistence).  Again, we could cut corners by sending out questionnaires, but return rates will be low without personal contact, and there's always an inverse relationshp between depth (i.e., usefulness) and response rate.  And even short, shallow questionnaires need significant investment if they're to be robust, so we wouldn't really be saving that much.

The final consideration is perhaps the most important: we want to discover phenomena that we don't yet know to look for, and which may not be mentioned by the participants in the course.  Greg could not have predicted that Software Carpentry's innovative teaching practices would do as much to convince scientists to take its lessons seriously as those lessons' content; he equally could not have predicted the importance of having people sign up for the class with their labmates, how compelling seemingly trivial things like tab completion and the 'history' command are to novices, or that the impedance mismatch between Microsoft Office file formats and version control systems is probably the single biggest barrier to wider uptake of the latter in the life sciences.  Those game-changing insights are what we think you want, and what science really needs; that's why we've designed our study to produce them.
