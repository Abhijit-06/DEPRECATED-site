---
layout: blog
root: ../../..
author: Greg Wilson
title: "Learning in Both Directions"
date: 2015-04-19
time: "12:00:00"
category: ["Opinion", "Assessment"]
---
<!-- start excerpt -->
<p>
  As regular readers will know,
  we have spent a lot of time thinking about how to assess the impact that Software Carpentry is having.
  We've done
  <a href="{{page.root}}/bib/aranda-assessment-2012-07.pdf">some</a>
  <a href="{{page.root}}/bib/libarkin-assessment-report-2012-06.pdf">small</a>
  <a href="http://arxiv.org/abs/1407.6220">studies</a>,
  and collected <a href="{{page.root}}/pages/testimonials.html">a few testimonials</a>,
  but it's always felt like small potatoes compared to the 5000 people we taught last year alone.
</p>
<p>
  After some back and forth with a colleague whose work I have admired for years,
  though,
  I've come to realize that I may have been trying to go about this the wrong way.
  Having trained as an engineer,
  I used to believe that only controlled, quantitative experiments were "real" science&mdash;that
  as Ernest Rutherford said,
  it's either physics or stamp collecting.
  I now understand that there are other rigorous ways to generate actionable insights,
  and that some of those methods are better suited to our needs than something like randomized control trials.
  More than that,
  I finally understand what one of my first teachers told me:
</p>
<blockquote>
  <p>
    Teaching only works well when the teacher is also learning.
  </p>
</blockquote>
<!-- end excerpt -->
<p>
  Let's start with assessment.
  I used to think that what we needed was something like a medical trial:
  give some people training,
  don't give it to others,
  then measure how much science the treatment group does compared to the control group.
  The problems with that are:
</p>
<ol>
  <li>
    we don't know how to measure the productivity of scientists, and
  </li>
  <li>
    what we really care about is culture change.
  </li>
</ol>
<p>
  We need to look at learning outcomes,
  but we already know what the answer will be:
  some people are learning some of what we teach,
  and some of them are then using it in ways that change how they do science.
  Even if we could quantify that,
  what we really want to know is not
  "how much Git or SQL do participants remember a month later?"
  but
  "how does knowing about version control and structured data change what scientists do and how they do it?"
  Randomised trials of course participants aren't going to show this:
  evaluating the adoption of practices by scientists and scientific teams requires a rigorous <em>qualitative</em> investigation.
</p>
<p>
  An example of what I have in mind is Marian Petre's award-winning paper
  "<a href="http://oro.open.ac.uk/35805/8/UML%20in%20practice%208.pdf">UML in Practice</a>".
  For those who haven't run into it,
  UML (the Unified Modeling Language) is a graphical notation for describing software.
  It was created in the mid-1990s by amalgamating three earlier design notations,
  and while it has been widely adopted in academia&mdash;almost every undergraduate textbook on software engineering
  devotes at least a chapter to it&mdash;there has been much less uptake in industry.
  As part of her long-running investigation of how people actually build software,
  Petre interviewed 50 professional software developers over two years
  to find out whether they were using UML:
  or rather,
  which bits of UML they were using, and how, and most importantly, <em>why</em>.
</p>
<p>
  The results are fascinating (in the usual academic sense of that word).
  Thirty-five of her interviewees don't use UML at all,
  but their reasons for not using it are varied.
  Those who <em>do</em> use it only use parts,
  with varying degrees of formality,
  and in idiosyncratic ways.
  Questionnaires and objective performance metrics wouldn't have revealed this,
  but it's exactly what we need to know if we want to understand adoption into practice,
  knowledge exchange in a new community,
  and everything else that actually matters.
</p>
<p>
  The real goal is to discover phenomena that we don't yet know to look for.
  I could not have predicted that Software Carpentry's innovative teaching practices
  would do as much to convince scientists to take its lessons seriously as those lessons' content;
  I equally could not have predicted the importance of having people sign up for the class with their labmates,
  how compelling seemingly trivial things like tab completion and the 'history' command are to novices,
  or that the impedance mismatch between Microsoft Office file formats and version control systems
  is probably the single biggest barrier to wider uptake of the latter in the life sciences.
  Given how little funding there is for assessment,
  we need to spend our dollars and euros on things that are most likely to produce those game-changing insights.
</p>
<p>
  I quoted Ernest Rutherford at the outset of this article,
  but in checking that quote,
  I discovered that I'd been getting it wrong for years.
  I thought he'd said,
  "All science is either physics or butterfly collecting."
  Those of us indoctrinated in quantitative methods may look down on their qualitative kin,
  but the fact is,
  the greatest scientific theory of all time&mdash;evolution by natural selection&mdash;didn't come out of
  double-blind controlled trials.
  It came out of butterfly collecting&mdash;that,
  and careful thinking about the butterflies that had been collected.
</p>
<p>
  I also quoted my father at the start of this article:
  "Teaching only works well when the teacher is also learning."
  I learned a lot about what to teach and how to teach it
  when we started running workshops for ecologists and marine biologists
  as well as physicists and astronomers.
  Now that we're starting to teach social scientists,
  I'm learning again,
  this time about the methods they use and how they could help us do what we really want to do:
  change the world.
</p>
